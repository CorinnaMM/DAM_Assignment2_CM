library(dplyr)
library(plyr)
library(ggplot2)
library(caret)
library(pls)
library(xgboost)
library(parallel)
library(doParallel)
# set up cluster for parallel computing
cluster = makeCluster(detectCores() - 1)
registerDoParallel(cluster)
setwd("~/Documents/GitHub/DAM_Assignment2_CM/01_Code")
raw_df = read.csv("~/Documents/GitHub/DAM_Assignment2_CM/02_Working_data_folder/clean_data_v5.csv")
df = raw_df
#basic exploration
head(df, 10)
tail(df, 10)
dim(df)
str(df)
summary(df)
#frequency of target and non-target variable
count(df$loan_status)
#check for duplicated values
sum(duplicated(df)==TRUE)
# Plot histogram grid
#factor.variables = sapply(df, is.factor)
#factors.df <- df[, factor.variables]
#factors.df
# 15 factor variables to loop through
#dim(factors.df)
#plot all factor data in grouped plot
#par(mfrow= c(4,4))
#vars = 15
#startVar = 1
#while(startVar <= vars ){
#  barplot(prop.table(table(factors.df[,startVar])), main=colnames(factors.df)[startVar],las = 2)
#  startVar = startVar + 1
#}
#plot each graph separately
#par(mfrow= c(1,1))
#vars = 15
#startVar = 1
#while(startVar <= vars ){
#  barplot(prop.table(table(factors.df[,startVar])), main=colnames(factors.df)[startVar],las = 2)
#  startVar = startVar + 1
#}
pca_var <- c(2,3,4,7,25:28)
pcadf = df[,pca_var]
str(df)
str(pcadf)
pr_out = prcomp(df[,pca_var], scale = T)
names(pr_out)
pr_out$rotation
#biplot(pr_out)
#scree plot
pr_var = pr_out$sdev ^ 2
pve = pr_var/sum(pr_var)
plot(pve, type = "b", main ="Scree Plot",
ylab = "Proportion of Variance Explained",
xlab = "Principal Component")
plot(cumsum(pve), type = 'b', main = "Cumulative Variance Explained",
xlab = "Number of Components",
ylab = "Cumulative Proportion of Variance Explained")
#come back to this and revaluate groupings
#=====================================================
# XGBoost Model w/ k = 5 fold cv plus grid search
#=====================================================
set.seed(42)
train = createDataPartition(y = df$loan_status, p = 0.7, list = F)
# drop ID column
training = df[train,]
testing = df[-train,]
dim(training)
dim(testing)
xgb_control = trainControl(method = "cv",
number = 5,
classProbs = T,
summaryFunction = twoClassSummary,
allowParallel = TRUE
)
# XGB Hyperparams
# tuning params for XGB best practice https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
xgb_hyperparams = expand.grid(nrounds = 1000,
eta = c(0.01, 0.001, 0.001),
max_depth = c(2,4,6,8,10),
gamma = c(0,1), #default = 0
colsample_bytree = c(0.6,0.8,1),
min_child_weight = 1,
subsample = c(0.5,0.75,1)
)
# Consider scaling this back as model takes a while to train
#xgb_hyperparams = expand.grid(nrounds = 1000,
#                              eta = c(0.01, 0.001, 0.001),
#                              max_depth = c(2,4,6,8,10),
#                              gamma = c(0,1), #default = 0
#                              colsample_bytree = c(0.6,0.8,1),
#                              min_child_weight = 1,
#                              subsample = c(0.5,0.75,1)
#)
x_train <- model.matrix( ~ ., training[,-14])
xgb_fit = train(x = x_train, y = training$loan_status,
method='xgbTree',
trControl= trainControl,
tuneGrid = xgb_hyperparams,
metric = 'ROC')
xgb_fit = train(x = x_train, y = as.factor(training$loan_status),
method='xgbTree',
trControl= trainControl,
tuneGrid = xgb_hyperparams,
metric = 'ROC')
x_train <- model.matrix( ~ ., training[,-14])
head(training)
xgb_hyperparams = expand.grid(nrounds = 1000,
eta = c(0.01, 0.001, 0.001),
max_depth = c(2,4,6,8,10),
gamma = c(0,1), #default = 0
colsample_bytree = c(0.6,0.8,1),
min_child_weight = 1,
subsample = 0.5
)
xgb_fit = train(x = x_train, y = as.factor(training$loan_status),
method='xgbTree',
trControl= trainControl,
tuneGrid = xgb_hyperparams,
metric = 'ROC')
xgb_fit = train(x = x_train, y = training$loan_status,
method='xgbTree',
trControl= xgb_control,
tuneGrid = xgb_hyperparams,
metric = 'ROC')
xgb_fit$results
print(xgb_fit)
